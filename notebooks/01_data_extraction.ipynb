{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcef160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import praw\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "REDDIT_CLIENT_ID = 'brWU7p9j_4pulCPxMK2fqw'\n",
    "REDDIT_CLIENT_SECRET = 'SYu7-3f0iUl8QRHJvgo9X80T4efBPg'\n",
    "REDDIT_USERNAME = 'Careful-Relation-574'\n",
    "REDDIT_PASSWORD = 'TbQPu6GVbGqbQC_'\n",
    "USER_AGENT = 'reddit_data_collector by /u/Careful-Relation-574'\n",
    "\n",
    "output_path = OUTPUT_JSON = \"../data/raw/data.json\"\n",
    "\n",
    "LIMIT_PER_SEARCH = 1500\n",
    "SLEEP_BETWEEN_QUERIES = 1.0\n",
    "RANDOM_STATE = 42\n",
    "# ----------------------------------------\n",
    "\n",
    "random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8432ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If file exists, load it, else start fresh\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_posts = json.load(f)\n",
    "else:\n",
    "    all_posts = {}\n",
    "\n",
    "print(f\"Loaded {len(all_posts)} posts already in dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6c3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "    \"depression\", \"depressed\", \"mentalhealth\",\n",
    "    \"offmychest\", \"TrueOffMyChest\", \"confession\", \"self\",\n",
    "    \"stress\", \"lonely\", \"sad\", \"burnout\", \"Anxiety\",\n",
    "    \"SuicideWatch\", \"depression_help\", \"kindvoice\", \"needadvice\",\n",
    "    \"relationships\", \"bipolarreddit\", \"traumatoolbox\"\n",
    "]\n",
    "\n",
    "suicidal_keywords = [\n",
    "    \"suicidal\", \"i am suicidal\", \"i'm suicidal\", \"want to die\", \"want to end it all\",\n",
    "    \"kill myself\", \"i will kill myself\", \"i want to die\", \"end it all\", \"ending my life\",\n",
    "    \"i cant take it\", \"i can't take it\", \"i don't want to live\", \"dont want to live\",\n",
    "    \"dont want to be here\", \"i give up\", \"ready to die\", \"wish i was dead\", \"better off dead\",\n",
    "    \"thinking about suicide\", \"thoughts of suicide\", \"i want to end it\", \"i'm done\",\n",
    "    \"im done\", \"going to end it\", \"overdose\", \"want to disappear\", \"no reason to live\",\n",
    "    \"i want out\", \"i want to die right now\"\n",
    "]\n",
    "\n",
    "depression_keywords = [\n",
    "    \"depressed\", \"feeling depressed\", \"i feel depressed\", \"feeling low\", \"im feeling down\",\n",
    "    \"i'm feeling down\", \"feeling down\", \"low mood\", \"hopeless\", \"hopelessness\", \"worthless\",\n",
    "    \"i hate myself\", \"i'm broken\", \"i am broken\", \"i'm lonely\", \"im lonely\", \"numb\",\n",
    "    \"nothing matters\", \"i'm so sad\", \"i am so sad\", \"i'm so tired\", \"i'm tired of living\",\n",
    "    \"crying again\", \"i cry\", \"feeling empty\", \"lost interest\", \"no motivation\", \"can't cope\",\n",
    "    \"cant cope\", \"can't enjoy\", \"cant enjoy\"\n",
    "]\n",
    "\n",
    "anxiety_keywords = [\n",
    "    \"anxious\", \"feeling anxious\", \"i'm anxious\", \"im anxious\", \"panic attack\",\n",
    "    \"panic attacks\", \"panic\", \"overwhelmed\", \"i'm overwhelmed\", \"im overwhelmed\",\n",
    "    \"heart racing\", \"racing heart\", \"cant breathe\", \"can't breathe\", \"hyperventilate\",\n",
    "    \"overthinking\", \"worrying\", \"constant worry\", \"nervous\", \"social anxiety\",\n",
    "    \"can't sleep\", \"insomnia\", \"sweating\", \"shaking\", \"feeling panicked\"\n",
    "]\n",
    "\n",
    "# Normalize\n",
    "suicidal_keywords = [k.lower() for k in suicidal_keywords]\n",
    "depression_keywords = [k.lower() for k in depression_keywords]\n",
    "anxiety_keywords = [k.lower() for k in anxiety_keywords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37fa0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pattern(words):\n",
    "    return re.compile(\"|\".join(re.escape(w) for w in words), re.IGNORECASE)\n",
    "\n",
    "pattern_suicidal = make_pattern(suicidal_keywords)\n",
    "pattern_depression = make_pattern(depression_keywords)\n",
    "pattern_anxiety = make_pattern(anxiety_keywords)\n",
    "\n",
    "def label_text(text):\n",
    "    text = text.lower()\n",
    "    if pattern_suicidal.search(text):\n",
    "        return \"suicidal\"\n",
    "    elif pattern_depression.search(text):\n",
    "        return \"depression\"\n",
    "    elif pattern_anxiety.search(text):\n",
    "        return \"anxiety\"\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa69170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Reddit\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    username=REDDIT_USERNAME,\n",
    "    password=REDDIT_PASSWORD,\n",
    "    user_agent=USER_AGENT\n",
    ")\n",
    "print(\"✅ Connected to Reddit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a48046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subreddits: 100%|██████████| 19/19 [2:44:29<00:00, 519.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total collected: 101345 (suicidal / depression / anxiety only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sub in tqdm(subreddits, desc=\"Subreddits\"):\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    for kw in (suicidal_keywords + depression_keywords + anxiety_keywords):\n",
    "        try:\n",
    "            for submission in subreddit.search(kw, limit=LIMIT_PER_SEARCH):\n",
    "                pid = submission.id\n",
    "                if pid not in all_posts:\n",
    "                    text = f\"{submission.title} {submission.selftext}\"\n",
    "                    label = label_text(text)\n",
    "                    if label is None:   # 🚨 skip if not suicidal / depression / anxiety\n",
    "                        continue\n",
    "                    all_posts[pid] = {\n",
    "                        \"id\": pid,\n",
    "                        \"subreddit\": sub,\n",
    "                        \"title\": submission.title,\n",
    "                        \"selftext\": submission.selftext,\n",
    "                        \"label\": label,\n",
    "                        \"created_utc\": submission.created_utc,\n",
    "                        \"url\": submission.url\n",
    "                    }\n",
    "            time.sleep(SLEEP_BETWEEN_QUERIES)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error in r/{sub} kw={kw[:20]}... : {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Total collected: {len(all_posts)} (suicidal / depression / anxiety only)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd7ab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 101345 posts to ../data/raw/reddit_labeled.json\n"
     ]
    }
   ],
   "source": [
    "# os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(list(all_posts.values()), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Saved {len(all_posts)} posts to {OUTPUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d90c8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 101345 posts from ../data/raw/data.json\n",
      "Unique IDs loaded into memory: 101345\n"
     ]
    }
   ],
   "source": [
    "json_file = \"../data/raw/data.json\"\n",
    "\n",
    "# Load existing posts\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    existing_data = json.load(f)\n",
    "\n",
    "# Build set of already saved IDs to avoid duplicates\n",
    "existing_ids = {post[\"id\"] for post in existing_data}\n",
    "\n",
    "print(f\"Loaded {len(existing_data)} posts from {json_file}\")\n",
    "print(f\"Unique IDs loaded into memory: {len(existing_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0098eaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting normal posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching normal posts: 100%|██████████| 22/22 [05:02<00:00, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 35 normal posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------- Collect NORMAL posts --------\n",
    "print(\"Collecting normal posts...\")\n",
    "\n",
    "normal_subs = [\n",
    "    \"AskReddit\", \"CasualConversation\", \"Showerthoughts\", \"NoStupidQuestions\",\n",
    "    \"todayilearned\", \"explainlikeimfive\", \"AskScience\", \"AskHistorians\",\n",
    "    \"funny\", \"pics\", \"aww\", \"memes\",\n",
    "    \"food\", \"Cooking\", \"travel\", \"books\", \"movies\", \"television\", \"Music\",\n",
    "    \"gaming\", \"sports\", \"fitness\"\n",
    "]\n",
    "\n",
    "normal_posts = []\n",
    "\n",
    "for sub in tqdm(normal_subs, desc=\"Fetching normal posts\"):\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    for post in subreddit.hot(limit=5000):  # adjust limit if needed\n",
    "        if post.id in existing_ids:\n",
    "            continue  # avoid duplicates\n",
    "\n",
    "        text = f\"{post.title} {post.selftext}\".strip().lower()\n",
    "\n",
    "        # Skip if text contains depression/anxiety/suicidal keywords\n",
    "        if (pattern_suicidal.search(text) or \n",
    "            pattern_depression.search(text) or \n",
    "            pattern_anxiety.search(text)):\n",
    "            continue\n",
    "\n",
    "        normal_posts.append({\n",
    "            \"id\": post.id,\n",
    "            \"subreddit\": sub,\n",
    "            \"title\": post.title,\n",
    "            \"selftext\": post.selftext,\n",
    "            \"label\": \"normal\",\n",
    "            \"created_utc\": float(post.created_utc),\n",
    "            \"url\": f\"https://www.reddit.com{post.permalink}\"\n",
    "        })\n",
    "        existing_ids.add(post.id)\n",
    "\n",
    "print(f\"Collected {len(normal_posts)} normal posts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49b8badf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset → ../data/raw/data.json\n",
      "Final label counts: {'suicidal': 33736, 'depression': 36884, 'anxiety': 30725, 'normal': 11203}\n"
     ]
    }
   ],
   "source": [
    "# -------- Append normal posts to existing JSON --------\n",
    "\n",
    "# Path to previously saved file\n",
    "json_file = \"../data/raw/data.json\"\n",
    "\n",
    "# Load existing dataset\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    existing_data = json.load(f)\n",
    "\n",
    "# Append normal posts\n",
    "existing_data.extend(normal_posts)\n",
    "\n",
    "# Save back to the same file\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(existing_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Show updated label counts\n",
    "label_counts = {lbl: sum(1 for p in existing_data if p['label'] == lbl) \n",
    "                for lbl in ['suicidal', 'depression', 'anxiety', 'normal']}\n",
    "print(f\"Updated dataset → {json_file}\")\n",
    "print(\"Final label counts:\", label_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_balancing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
