{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bd9dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      " RAW_JSON: ../data/raw/data.json\n",
      " OUTPUT_CSV: ../data/processed/balanced_dataset.csv\n",
      " Random seed: 42\n",
      " Target range per label: 11000 - 11500\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Imports & CONFIG\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "RAW_JSON = \"../data/raw/data.json\"                  # input (adjust if needed)\n",
    "OUTPUT_CSV = \"../data/processed/balanced_dataset.csv\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# target range (11k - 11.5k)\n",
    "TARGET_MIN = 11000\n",
    "TARGET_MAX = 11500\n",
    "\n",
    "# text-length buckets (words)\n",
    "SHORT_MAX = 19      # <20 words -> short\n",
    "MEDIUM_MAX = 100    # 20-100 words -> medium\n",
    "# long -> >100 words\n",
    "\n",
    "# minimal combined text length in characters to keep (very short posts removed)\n",
    "MIN_CHARS_KEEP = 20\n",
    "\n",
    "print(\"Config:\")\n",
    "print(\" RAW_JSON:\", RAW_JSON)\n",
    "print(\" OUTPUT_CSV:\", OUTPUT_CSV)\n",
    "print(\" Random seed:\", RANDOM_STATE)\n",
    "print(\" Target range per label:\", TARGET_MIN, \"-\", TARGET_MAX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e093c450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw entries: 122534\n",
      "Columns available: ['id', 'subreddit', 'title', 'selftext', 'label', 'created_utc', 'url']\n",
      "Initial label distribution:\n",
      "Counter({'depression': 36884, 'suicidal': 33736, 'anxiety': 30725, 'normal': 21189})\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Load raw JSON and show counts\n",
    "with open(RAW_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "print(\"Total raw entries:\", len(raw))\n",
    "\n",
    "df = pd.DataFrame(raw)\n",
    "# Ensure columns exist\n",
    "for col in [\"id\",\"title\",\"selftext\",\"label\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"\n",
    "\n",
    "print(\"Columns available:\", df.columns.tolist())\n",
    "print(\"Initial label distribution:\")\n",
    "print(Counter(df[\"label\"].fillna(\"missing\").values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "724860d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping rows where title OR selftext is empty: 114872\n",
      "After removing combined length < 20 chars: 114860\n",
      "After dropping duplicate ids: 114860\n",
      "After dropping duplicates by normalized text: 114077\n",
      "Label counts after cleaning & de-dup:\n",
      "  suicidal: 32795\n",
      "  depression: 36048\n",
      "  anxiety: 29770\n",
      "  normal: 15464\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Cleaning helpers and execution\n",
    "\n",
    "def normalize_whitespace(s):\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "_trans_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def normalize_text_for_dup(s):\n",
    "    \"\"\"Normalize text for duplicate detection: lowercase, remove punctuation, collapse whitespace.\"\"\"\n",
    "    s = s.lower()\n",
    "    s = s.translate(_trans_table)   # remove punctuation\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# 1) Ensure title & selftext are strings\n",
    "df[\"title\"] = df[\"title\"].fillna(\"\").astype(str).map(normalize_whitespace)\n",
    "df[\"selftext\"] = df[\"selftext\"].fillna(\"\").astype(str).map(normalize_whitespace)\n",
    "\n",
    "# 2) Drop rows where EITHER title OR selftext is empty (user requested strict rule)\n",
    "mask_keep = (df[\"title\"].str.strip() != \"\") & (df[\"selftext\"].str.strip() != \"\")\n",
    "df = df[mask_keep].reset_index(drop=True)\n",
    "print(\"After dropping rows where title OR selftext is empty:\", len(df))\n",
    "\n",
    "# 3) Create combined text and drop extremely short content\n",
    "df[\"combined\"] = (df[\"title\"] + \" \" + df[\"selftext\"]).str.strip()\n",
    "df = df[df[\"combined\"].str.len() >= MIN_CHARS_KEEP].reset_index(drop=True)\n",
    "print(f\"After removing combined length < {MIN_CHARS_KEEP} chars:\", len(df))\n",
    "\n",
    "# 4) Drop duplicate IDs (keep first)\n",
    "df = df.drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "print(\"After dropping duplicate ids:\", len(df))\n",
    "\n",
    "# 5) Drop duplicates by normalized combined text\n",
    "df[\"norm_combined\"] = df[\"combined\"].map(normalize_text_for_dup)\n",
    "df = df.drop_duplicates(subset=[\"norm_combined\"]).reset_index(drop=True)\n",
    "print(\"After dropping duplicates by normalized text:\", len(df))\n",
    "\n",
    "# Final counts\n",
    "print(\"Label counts after cleaning & de-dup:\")\n",
    "label_counts = Counter(df[\"label\"].values)\n",
    "for k,v in label_counts.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d23bd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket distribution (overall):\n",
      "bucket\n",
      "long      82173\n",
      "medium    27858\n",
      "short      4046\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Assign text-length buckets and define sampling function\n",
    "\n",
    "def word_count_bucket(s):\n",
    "    wc = len(s.split())\n",
    "    if wc <= SHORT_MAX:\n",
    "        return \"short\"\n",
    "    elif wc <= MEDIUM_MAX:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"long\"\n",
    "\n",
    "df[\"wc\"] = df[\"combined\"].map(lambda s: len(s.split()))\n",
    "df[\"bucket\"] = df[\"combined\"].map(word_count_bucket)\n",
    "\n",
    "print(\"Bucket distribution (overall):\")\n",
    "print(df[\"bucket\"].value_counts())\n",
    "\n",
    "# sampling helper: proportional by bucket within a class\n",
    "def stratified_sample_by_buckets(class_df, target, seed=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    class_df: DataFrame for one label with 'bucket' column\n",
    "    target: desired number of samples (<= len(class_df))\n",
    "    Returns sampled DataFrame of size 'target'.\n",
    "    \"\"\"\n",
    "    if len(class_df) <= target:\n",
    "        # nothing to do\n",
    "        return class_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # compute bucket proportions\n",
    "    bucket_counts = class_df[\"bucket\"].value_counts().to_dict()\n",
    "    total = sum(bucket_counts.values())\n",
    "    # initial bucket targets (rounded)\n",
    "    bucket_targets = {b: max(0, int(round((cnt/total)*target))) for b,cnt in bucket_counts.items()}\n",
    "\n",
    "    # adjust rounding to sum to target\n",
    "    assigned = sum(bucket_targets.values())\n",
    "    # if assigned != target, adjust by adding/subtracting 1 to buckets by largest fractional remainders\n",
    "    if assigned != target:\n",
    "        # compute fractional parts exactly\n",
    "        fractions = {b: (bucket_counts[b]/total)*target - bucket_targets[b] for b in bucket_counts}\n",
    "        # sort buckets by descending fractional part if we need to add, ascending if subtract\n",
    "        diff = target - assigned\n",
    "        if diff > 0:\n",
    "            order = sorted(fractions.items(), key=lambda x: -x[1])\n",
    "            i = 0\n",
    "            while diff > 0:\n",
    "                bucket_targets[order[i % len(order)][0]] += 1\n",
    "                diff -= 1\n",
    "                i += 1\n",
    "        elif diff < 0:\n",
    "            order = sorted(fractions.items(), key=lambda x: x[1])\n",
    "            i = 0\n",
    "            while diff < 0:\n",
    "                b = order[i % len(order)][0]\n",
    "                if bucket_targets[b] > 0:\n",
    "                    bucket_targets[b] -= 1\n",
    "                    diff += 1\n",
    "                i += 1\n",
    "\n",
    "    # Now sample per bucket; if a bucket doesn't have enough, take all and reassign remainder\n",
    "    sampled_parts = []\n",
    "    remaining_to_fill = 0\n",
    "    for b, btarget in bucket_targets.items():\n",
    "        bucket_df = class_df[class_df[\"bucket\"] == b]\n",
    "        if len(bucket_df) >= btarget:\n",
    "            sampled_parts.append(bucket_df.sample(n=btarget, random_state=seed))\n",
    "        else:\n",
    "            # take all and accumulate deficit\n",
    "            sampled_parts.append(bucket_df)\n",
    "            remaining_to_fill += (btarget - len(bucket_df))\n",
    "\n",
    "    if remaining_to_fill > 0:\n",
    "        # collect pool of rows not already taken\n",
    "        taken_idx = pd.concat(sampled_parts).index\n",
    "        pool = class_df.drop(index=taken_idx)\n",
    "        if len(pool) >= remaining_to_fill:\n",
    "            sampled_parts.append(pool.sample(n=remaining_to_fill, random_state=seed))\n",
    "        else:\n",
    "            # fallback: sample with replacement from class_df to meet exact size\n",
    "            extra = class_df.sample(n=remaining_to_fill, replace=True, random_state=seed)\n",
    "            sampled_parts.append(extra)\n",
    "\n",
    "    result = pd.concat(sampled_parts, axis=0).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    # sanity: trim/extend to target if necessary\n",
    "    if len(result) > target:\n",
    "        result = result.sample(n=target, random_state=seed).reset_index(drop=True)\n",
    "    elif len(result) < target:\n",
    "        # append random rows with replacement\n",
    "        extra = class_df.sample(n=(target - len(result)), replace=True, random_state=seed)\n",
    "        result = pd.concat([result, extra], axis=0).reset_index(drop=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43f1d123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels present: ['anxiety', 'depression', 'normal', 'suicidal']\n",
      "Targets per label (will not upsample):\n",
      "{'anxiety': 11327, 'depression': 11057, 'normal': 11012, 'suicidal': 11379}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling labels:  25%|██▌       | 1/4 [00:00<00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  anxiety: downsampling 29770 -> 11327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling labels:  50%|█████     | 2/4 [00:00<00:00,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  depression: downsampling 36048 -> 11057\n",
      "  normal: downsampling 15464 -> 11012\n",
      "  suicidal: downsampling 32795 -> 11379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling labels: 100%|██████████| 4/4 [00:00<00:00,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined balanced size: 44775\n",
      "Label distribution after sampling:\n",
      "Counter({'suicidal': 11379, 'anxiety': 11327, 'depression': 11057, 'normal': 11012})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Compute a target per label (random in [TARGET_MIN, TARGET_MAX]) and perform stratified sampling\n",
    "labels = sorted(df[\"label\"].unique())\n",
    "print(\"Labels present:\", labels)\n",
    "\n",
    "label_targets = {}\n",
    "for lbl in labels:\n",
    "    # choose target in range\n",
    "    t = random.randint(TARGET_MIN, TARGET_MAX)\n",
    "    # safety: if we have fewer than t in raw, set t to available (we only downsample here)\n",
    "    available = int((df[\"label\"] == lbl).sum())\n",
    "    if available < t:\n",
    "        t = available\n",
    "    label_targets[lbl] = t\n",
    "\n",
    "print(\"Targets per label (will not upsample):\")\n",
    "print(label_targets)\n",
    "\n",
    "# perform sampling\n",
    "sampled_parts = []\n",
    "for lbl in tqdm(labels, desc=\"Sampling labels\"):\n",
    "    class_df = df[df[\"label\"] == lbl].reset_index(drop=True)\n",
    "    target = label_targets[lbl]\n",
    "    if len(class_df) <= target:\n",
    "        print(f\"  {lbl}: has {len(class_df)} <= target {target}, keeping all\")\n",
    "        sampled = class_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    else:\n",
    "        print(f\"  {lbl}: downsampling {len(class_df)} -> {target}\")\n",
    "        sampled = stratified_sample_by_buckets(class_df, target, seed=RANDOM_STATE)\n",
    "    sampled_parts.append(sampled)\n",
    "\n",
    "balanced_df = pd.concat(sampled_parts, axis=0).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "print(\"Combined balanced size:\", len(balanced_df))\n",
    "print(\"Label distribution after sampling:\")\n",
    "print(Counter(balanced_df[\"label\"].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84a7dfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size after ensuring unique ids: 44775\n",
      "Saved balanced dataset to: ../data/processed/balanced_dataset.csv\n",
      "Final counts per label:\n",
      "Counter({'suicidal': 11379, 'anxiety': 11327, 'depression': 11057, 'normal': 11012})\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Finalize columns and save CSV with only id, title, selftext, label\n",
    "final_df = balanced_df[[\"id\", \"title\", \"selftext\", \"label\"]].copy()\n",
    "\n",
    "# Optional: ensure ids unique (safety)\n",
    "final_df = final_df.drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "print(\"Final dataset size after ensuring unique ids:\", len(final_df))\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved balanced dataset to:\", OUTPUT_CSV)\n",
    "\n",
    "# Print final counts\n",
    "print(\"Final counts per label:\")\n",
    "print(Counter(final_df[\"label\"].values))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_balancing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
